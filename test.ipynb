{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import lightning as L\n",
    "from config import load_config\n",
    "from models.classifier import Classifier\n",
    "from datasets import instentiate_dataloader\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "cfg = \"lightning_logs/version_166775/hparams.yaml\"\n",
    "new_checkpoint = \"lightning_logs/version_166775/checkpoints/epoch=199-step=17000.ckpt\"\n",
    "checkpoint = \"lightning_logs/version_166775/VIT_B_16.ckpt\"\n",
    "cfg = load_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = torch.load(checkpoint, map_location=\"cpu\")\n",
    "for param in tm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = Classifier(cfg)\n",
    "\n",
    "if new_checkpoint:\n",
    "    model = Classifier.load_from_checkpoint(new_checkpoint)\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./data/flowers-17/jpg/image_0376.jpg\")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "x = transform(img)\n",
    "\n",
    "# out, att_mat = tm.get_submodule(\"backbone\")(x.unsqueeze(0))\n",
    "out, att_mat = model.backbone(x.unsqueeze(0))\n",
    "\n",
    "att_mat = [x.unsqueeze(0) for x in att_mat]\n",
    "\n",
    "att_mat = torch.stack(att_mat).squeeze(1)\n",
    "\n",
    "att_mat = torch.mean(att_mat, dim=1)\n",
    "\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "for i, v in enumerate(joint_attentions):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    # Attention from the output token to the input space.\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "    mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n",
    "    red_mask = np.zeros_like(mask)\n",
    "    red_mask[..., 0] = 255  # Set the red channel to 255\n",
    "    result = (mask * img + red_mask).astype(\"uint8\")\n",
    "\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "    ax1.set_title('Original')\n",
    "    ax2.set_title('Attention Map_%d Layer' % (i+1))\n",
    "    _ = ax1.imshow(img)\n",
    "    _ = ax2.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3340",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
